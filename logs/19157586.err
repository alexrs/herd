Loaded module: cuda/11.6
Loaded dependency [python3/3.11.4]: gcc/12.3.0-binutils-2.40
Loaded dependency [python3/3.11.4]: sqlite3/3.42.0
Loaded module: python3/3.11.4

Loading python3/3.11.4
  Loading requirement: gcc/12.3.0-binutils-2.40 sqlite3/3.42.0
Found cached dataset parquet (/work3/s212722/herd/datasets/cache/alexrs___parquet/alexrs--alpaca-cleaned-10-clusters-e0d2361138237477/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
Loading cached processed dataset at /work3/s212722/herd/datasets/cache/alexrs___parquet/alexrs--alpaca-cleaned-10-clusters-e0d2361138237477/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-dd77dbeeee1b1a95.arrow
2023-11-05 21:44:07.491 | INFO     | herd.finetune_utils:get_peft_config:30 - Fine-tuning with MoLoRA using 10 experts
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [16:22<16:22, 982.71s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [21:57<00:00, 601.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [21:57<00:00, 658.78s/it]
/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.
  warnings.warn(
2023-11-05 22:06:29.259 | INFO     | herd.finetune:_finetune_experts:235 - Training expert: expert_0, output_dir: /work3/s212722/herd/meta-llama/Llama-2-7b-hf/alpaca-10/q_v_r4/molora/expert_0
wandb: Currently logged in as: alexrs95. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /zhome/03/c/164482/code/herd/wandb/run-20231105_220632-mn1foz5s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run molora/expert_0
wandb: â­ï¸ View project at https://wandb.ai/alexrs95/herd-llama
wandb: ðŸš€ View run at https://wandb.ai/alexrs95/herd-llama/runs/mn1foz5s
  0%|          | 0/2464 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
  0%|          | 1/2464 [00:27<19:05:07, 27.90s/it]  0%|          | 2/2464 [00:46<15:19:32, 22.41s/it]  0%|          | 3/2464 [01:04<13:57:47, 20.43s/it]  0%|          | 4/2464 [01:22<13:21:14, 19.54s/it]  0%|          | 5/2464 [01:40<13:02:06, 19.08s/it]  0%|          | 6/2464 [01:59<12:51:05, 18.82s/it]  0%|          | 7/2464 [02:17<12:44:15, 18.66s/it]  0%|          | 8/2464 [02:35<12:39:58, 18.57s/it]  0%|          | 9/2464 [02:54<12:36:52, 18.50s/it]  0%|          | 10/2464 [03:12<12:34:44, 18.45s/it]                                                      0%|          | 10/2464 [03:12<12:34:44, 18.45s/it]  0%|          | 11/2464 [03:31<12:32:50, 18.41s/it]  0%|          | 12/2464 [03:49<12:31:29, 18.39s/it]  1%|          | 13/2464 [04:07<12:30:11, 18.36s/it]  1%|          | 14/2464 [04:25<12:29:17, 18.35s/it]  1%|          | 15/2464 [04:44<12:28:44, 18.34s/it]  1%|          | 16/2464 [05:02<12:28:16, 18.34s/it]  1%|          | 17/2464 [05:20<12:27:42, 18.33s/it]  1%|          | 18/2464 [05:39<12:27:23, 18.33s/it]  1%|          | 19/2464 [05:57<12:27:00, 18.33s/it]  1%|          | 20/2464 [06:15<12:26:47, 18.33s/it]                                                      1%|          | 20/2464 [06:15<12:26:47, 18.33s/it]  1%|          | 21/2464 [06:34<12:26:28, 18.33s/it]  1%|          | 22/2464 [06:52<12:26:12, 18.33s/it]  1%|          | 23/2464 [07:10<12:25:44, 18.33s/it]  1%|          | 24/2464 [07:29<12:25:23, 18.33s/it]  1%|          | 25/2464 [07:47<12:25:06, 18.33s/it]  1%|          | 26/2464 [08:05<12:24:43, 18.33s/it]  1%|          | 27/2464 [08:24<12:24:26, 18.33s/it]  1%|          | 28/2464 [08:42<12:24:21, 18.33s/it]  1%|          | 29/2464 [09:00<12:24:05, 18.34s/it]  1%|          | 30/2464 [09:19<12:23:51, 18.34s/it]                                                      1%|          | 30/2464 [09:19<12:23:51, 18.34s/it]  1%|â–         | 31/2464 [09:37<12:23:33, 18.34s/it]  1%|â–         | 32/2464 [09:55<12:22:58, 18.33s/it]  1%|â–         | 33/2464 [10:14<12:22:41, 18.33s/it]  1%|â–         | 34/2464 [10:32<12:22:21, 18.33s/it]  1%|â–         | 35/2464 [10:50<12:21:56, 18.33s/it]  1%|â–         | 36/2464 [11:09<12:21:37, 18.33s/it]  2%|â–         | 37/2464 [11:27<12:21:12, 18.32s/it]  2%|â–         | 38/2464 [11:45<12:20:58, 18.33s/it]  2%|â–         | 39/2464 [12:04<12:20:39, 18.33s/it]  2%|â–         | 40/2464 [12:22<12:20:26, 18.33s/it]                                                      2%|â–         | 40/2464 [12:22<12:20:26, 18.33s/it]  2%|â–         | 41/2464 [12:40<12:20:10, 18.33s/it]  2%|â–         | 42/2464 [12:59<12:19:44, 18.33s/it]  2%|â–         | 43/2464 [13:17<12:19:18, 18.32s/it]  2%|â–         | 44/2464 [13:35<12:19:10, 18.33s/it]  2%|â–         | 45/2464 [13:54<12:18:50, 18.33s/it]  2%|â–         | 46/2464 [14:12<12:18:21, 18.32s/it]  2%|â–         | 47/2464 [14:30<12:17:59, 18.32s/it]  2%|â–         | 48/2464 [14:49<12:17:44, 18.32s/it]  2%|â–         | 49/2464 [15:07<12:17:33, 18.32s/it]  2%|â–         | 50/2464 [15:25<12:17:12, 18.32s/it]                                                      2%|â–         | 50/2464 [15:25<12:17:12, 18.32s/it]  2%|â–         | 51/2464 [15:44<12:16:56, 18.32s/it]  2%|â–         | 52/2464 [16:02<12:16:40, 18.33s/it]  2%|â–         | 53/2464 [16:20<12:16:25, 18.33s/it]  2%|â–         | 54/2464 [16:39<12:16:08, 18.33s/it]  2%|â–         | 55/2464 [16:57<12:15:45, 18.33s/it]  2%|â–         | 56/2464 [17:15<12:15:35, 18.33s/it]  2%|â–         | 57/2464 [17:34<12:15:18, 18.33s/it]  2%|â–         | 58/2464 [17:52<12:14:55, 18.33s/it]  2%|â–         | 59/2464 [18:10<12:14:37, 18.33s/it]  2%|â–         | 60/2464 [18:29<12:14:20, 18.33s/it]                                                      2%|â–         | 60/2464 [18:29<12:14:20, 18.33s/it]  2%|â–         | 61/2464 [18:47<12:14:02, 18.33s/it]  3%|â–Ž         | 62/2464 [19:05<12:13:49, 18.33s/it]  3%|â–Ž         | 63/2464 [19:24<12:13:33, 18.33s/it]  3%|â–Ž         | 64/2464 [19:42<12:13:11, 18.33s/it]  3%|â–Ž         | 65/2464 [20:00<12:12:43, 18.33s/it]  3%|â–Ž         | 66/2464 [20:19<12:12:26, 18.33s/it]  3%|â–Ž         | 67/2464 [20:37<12:12:08, 18.33s/it]  3%|â–Ž         | 68/2464 [20:55<12:11:47, 18.33s/it]  3%|â–Ž         | 69/2464 [21:13<12:11:25, 18.32s/it]  3%|â–Ž         | 70/2464 [21:32<12:11:07, 18.32s/it]                                                      3%|â–Ž         | 70/2464 [21:32<12:11:07, 18.32s/it]  3%|â–Ž         | 71/2464 [21:50<12:10:45, 18.32s/it]  3%|â–Ž         | 72/2464 [22:08<12:10:26, 18.32s/it]  3%|â–Ž         | 73/2464 [22:27<12:10:12, 18.32s/it]  3%|â–Ž         | 74/2464 [22:45<12:09:58, 18.33s/it]  3%|â–Ž         | 75/2464 [23:03<12:09:43, 18.33s/it]  3%|â–Ž         | 76/2464 [23:22<12:09:11, 18.32s/it]  3%|â–Ž         | 77/2464 [23:40<12:08:57, 18.32s/it]  3%|â–Ž         | 78/2464 [23:58<12:08:42, 18.32s/it]  3%|â–Ž         | 79/2464 [24:17<12:08:13, 18.32s/it]  3%|â–Ž         | 80/2464 [24:35<12:07:45, 18.32s/it]                                                      3%|â–Ž         | 80/2464 [24:35<12:07:45, 18.32s/it]  3%|â–Ž         | 81/2464 [24:53<12:07:32, 18.32s/it]  3%|â–Ž         | 82/2464 [25:12<12:07:12, 18.32s/it]  3%|â–Ž         | 83/2464 [25:30<12:06:57, 18.32s/it]  3%|â–Ž         | 84/2464 [25:48<12:06:40, 18.32s/it]  3%|â–Ž         | 85/2464 [26:07<12:06:26, 18.32s/it]  3%|â–Ž         | 86/2464 [26:25<12:06:05, 18.32s/it]  4%|â–Ž         | 87/2464 [26:43<12:05:49, 18.32s/it]  4%|â–Ž         | 88/2464 [27:02<12:05:25, 18.32s/it]  4%|â–Ž         | 89/2464 [27:20<12:05:05, 18.32s/it]  4%|â–Ž         | 90/2464 [27:38<12:04:42, 18.32s/it]                                                      4%|â–Ž         | 90/2464 [27:38<12:04:42, 18.32s/it]  4%|â–Ž         | 91/2464 [27:57<12:04:30, 18.32s/it]
Aborted!
Traceback (most recent call last):
  File "/zhome/03/c/164482/code/herd/main.py", line 78, in <module>
    main()
  File "/zhome/03/c/164482/code/herd/main.py", line 55, in main
    finetune(
  File "/zhome/03/c/164482/code/herd/herd/finetune.py", line 269, in finetune
    _finetune_experts(dataset, tokenizer, config, experts, peft_strategy, use_base, model_values, path_values)
  File "/zhome/03/c/164482/code/herd/herd/finetune.py", line 236, in _finetune_experts
    trainer.train()
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/transformers/trainer.py", line 1591, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/transformers/trainer.py", line 1892, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/transformers/trainer.py", line 2787, in training_step
    self.accelerator.backward(loss)
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/accelerate/accelerator.py", line 1853, in backward
    loss.backward(**kwargs)
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Terminated
