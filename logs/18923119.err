Loaded module: cuda/11.6
Loaded dependency [python3/3.11.4]: gcc/12.3.0-binutils-2.40
Loaded dependency [python3/3.11.4]: sqlite3/3.42.0
Loaded module: python3/3.11.4

Loading python3/3.11.4
  Loading requirement: gcc/12.3.0-binutils-2.40 sqlite3/3.42.0
Found cached dataset parquet (/work3/s212722/herd/datasets/cache/alexrs___parquet/alexrs--alpaca-cleaned-15-clusters-dc700effb75224a8/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
Filter:   0%|          | 0/51760 [00:00<?, ? examples/s]Filter:  19%|â–ˆâ–‰        | 10000/51760 [00:00<00:00, 94274.15 examples/s]Filter:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 20000/51760 [00:00<00:00, 49548.28 examples/s]Filter:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28000/51760 [00:00<00:00, 45496.20 examples/s]Filter:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 36000/51760 [00:00<00:00, 43895.70 examples/s]Filter:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 41000/51760 [00:00<00:00, 43253.60 examples/s]Filter:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 46000/51760 [00:01<00:00, 42707.64 examples/s]Filter:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 51000/51760 [00:01<00:00, 42508.78 examples/s]                                                                       2023-10-28 16:03:51.213 | INFO     | herd.finetune_utils:get_peft_config:30 - Fine-tuning with MoLoRA using 15 experts
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.26s/it]
2023-10-28 16:03:57.435 | WARNING  | herd.finetune:_finetune_experts:174 - Number of experts is not 1 but we are training an expert, setting it to 1. num_experts: 1
/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.
  warnings.warn(
2023-10-28 16:04:31.176 | INFO     | herd.finetune:_finetune_experts:206 - Training expert: expert_1, output_dir: /work3/s212722/herd/meta-llama/Llama-2-7b-hf/alpaca-15/all_linear_paged_adam_no_quant/molora/expert_1
wandb: Currently logged in as: alexrs95. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /zhome/03/c/164482/code/herd/wandb/run-20231028_160433-fatlwxnd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run molora/expert_1
wandb: â­ï¸ View project at https://wandb.ai/alexrs95/herd-llama
wandb: ðŸš€ View run at https://wandb.ai/alexrs95/herd-llama/runs/fatlwxnd
  0%|          | 0/1048 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
  0%|          | 1/1048 [00:12<3:42:32, 12.75s/it]  0%|          | 2/1048 [00:23<3:24:24, 11.73s/it]  0%|          | 3/1048 [00:34<3:18:41, 11.41s/it]  0%|          | 4/1048 [00:45<3:16:01, 11.27s/it]  0%|          | 5/1048 [00:56<3:14:33, 11.19s/it]  1%|          | 6/1048 [01:07<3:13:40, 11.15s/it]  1%|          | 7/1048 [01:19<3:13:04, 11.13s/it]  1%|          | 8/1048 [01:30<3:12:40, 11.12s/it]  1%|          | 9/1048 [01:41<3:12:24, 11.11s/it]  1%|          | 10/1048 [01:52<3:12:09, 11.11s/it]                                                     1%|          | 10/1048 [01:52<3:12:09, 11.11s/it]  1%|          | 11/1048 [02:03<3:11:57, 11.11s/it]  1%|          | 12/1048 [02:14<3:11:47, 11.11s/it]  1%|          | 13/1048 [02:25<3:11:37, 11.11s/it]  1%|â–         | 14/1048 [02:36<3:11:26, 11.11s/it]  1%|â–         | 15/1048 [02:47<3:11:15, 11.11s/it]  2%|â–         | 16/1048 [02:58<3:11:03, 11.11s/it]  2%|â–         | 17/1048 [03:10<3:10:53, 11.11s/it]  2%|â–         | 18/1048 [03:21<3:10:43, 11.11s/it]  2%|â–         | 19/1048 [03:32<3:10:31, 11.11s/it]  2%|â–         | 20/1048 [03:43<3:10:22, 11.11s/it]                                                     2%|â–         | 20/1048 [03:43<3:10:22, 11.11s/it]  2%|â–         | 21/1048 [03:54<3:10:12, 11.11s/it]  2%|â–         | 22/1048 [04:05<3:10:03, 11.11s/it]  2%|â–         | 23/1048 [04:16<3:09:50, 11.11s/it]  2%|â–         | 24/1048 [04:27<3:09:37, 11.11s/it]  2%|â–         | 25/1048 [04:38<3:09:25, 11.11s/it]  2%|â–         | 26/1048 [04:50<3:09:15, 11.11s/it]  3%|â–Ž         | 27/1048 [05:01<3:09:05, 11.11s/it]  3%|â–Ž         | 28/1048 [05:12<3:08:55, 11.11s/it]  3%|â–Ž         | 29/1048 [05:23<3:08:44, 11.11s/it]  3%|â–Ž         | 30/1048 [05:34<3:08:32, 11.11s/it]                                                     3%|â–Ž         | 30/1048 [05:34<3:08:32, 11.11s/it]  3%|â–Ž         | 31/1048 [05:45<3:08:22, 11.11s/it]  3%|â–Ž         | 32/1048 [05:56<3:08:13, 11.12s/it]  3%|â–Ž         | 33/1048 [06:07<3:08:01, 11.11s/it]  3%|â–Ž         | 34/1048 [06:19<3:07:49, 11.11s/it]  3%|â–Ž         | 35/1048 [06:30<3:07:38, 11.11s/it]  3%|â–Ž         | 36/1048 [06:41<3:07:25, 11.11s/it]  4%|â–Ž         | 37/1048 [06:52<3:07:14, 11.11s/it]  4%|â–Ž         | 38/1048 [07:03<3:07:06, 11.12s/it]  4%|â–Ž         | 39/1048 [07:14<3:06:54, 11.11s/it]  4%|â–         | 40/1048 [07:25<3:06:43, 11.11s/it]                                                     4%|â–         | 40/1048 [07:25<3:06:43, 11.11s/it]  4%|â–         | 41/1048 [07:36<3:06:31, 11.11s/it]  4%|â–         | 42/1048 [07:47<3:06:18, 11.11s/it]  4%|â–         | 43/1048 [07:59<3:06:06, 11.11s/it]  4%|â–         | 44/1048 [08:10<3:05:57, 11.11s/it]  4%|â–         | 45/1048 [08:21<3:05:47, 11.11s/it]  4%|â–         | 46/1048 [08:32<3:05:36, 11.11s/it]  4%|â–         | 47/1048 [08:43<3:05:26, 11.12s/it]  5%|â–         | 48/1048 [08:54<3:05:13, 11.11s/it]  5%|â–         | 49/1048 [09:05<3:05:01, 11.11s/it]  5%|â–         | 50/1048 [09:16<3:04:51, 11.11s/it]                                                     5%|â–         | 50/1048 [09:16<3:04:51, 11.11s/it]  5%|â–         | 51/1048 [09:27<3:04:40, 11.11s/it]  5%|â–         | 52/1048 [09:39<3:04:31, 11.12s/it]  5%|â–Œ         | 53/1048 [09:50<3:04:19, 11.12s/it]  5%|â–Œ         | 54/1048 [10:01<3:04:07, 11.11s/it]  5%|â–Œ         | 55/1048 [10:12<3:03:56, 11.11s/it]  5%|â–Œ         | 56/1048 [10:23<3:03:44, 11.11s/it]  5%|â–Œ         | 57/1048 [10:34<3:03:34, 11.12s/it]  6%|â–Œ         | 58/1048 [10:45<3:03:23, 11.12s/it]  6%|â–Œ         | 59/1048 [10:56<3:03:12, 11.11s/it]  6%|â–Œ         | 60/1048 [11:07<3:03:01, 11.12s/it]                                                     6%|â–Œ         | 60/1048 [11:07<3:03:01, 11.12s/it]  6%|â–Œ         | 61/1048 [11:19<3:02:49, 11.11s/it]  6%|â–Œ         | 62/1048 [11:30<3:02:39, 11.11s/it]  6%|â–Œ         | 63/1048 [11:41<3:02:26, 11.11s/it]  6%|â–Œ         | 64/1048 [11:52<3:02:16, 11.11s/it]  6%|â–Œ         | 65/1048 [12:03<3:02:03, 11.11s/it]  6%|â–‹         | 66/1048 [12:14<3:01:52, 11.11s/it]  6%|â–‹         | 67/1048 [12:25<3:01:41, 11.11s/it]  6%|â–‹         | 68/1048 [12:36<3:01:31, 11.11s/it]  7%|â–‹         | 69/1048 [12:48<3:01:22, 11.12s/it]  7%|â–‹         | 70/1048 [12:59<3:01:14, 11.12s/it]                                                     7%|â–‹         | 70/1048 [12:59<3:01:14, 11.12s/it]  7%|â–‹         | 71/1048 [13:10<3:01:04, 11.12s/it]  7%|â–‹         | 72/1048 [13:21<3:00:54, 11.12s/it]  7%|â–‹         | 73/1048 [13:32<3:00:41, 11.12s/it]  7%|â–‹         | 74/1048 [13:43<3:00:30, 11.12s/it]  7%|â–‹         | 75/1048 [13:54<3:00:19, 11.12s/it]  7%|â–‹         | 76/1048 [14:05<3:00:08, 11.12s/it]  7%|â–‹         | 77/1048 [14:15<2:53:55, 10.75s/it]  7%|â–‹         | 78/1048 [14:28<3:05:46, 11.49s/it]  8%|â–Š         | 79/1048 [14:40<3:03:41, 11.37s/it]  8%|â–Š         | 80/1048 [14:51<3:02:15, 11.30s/it]                                                     8%|â–Š         | 80/1048 [14:51<3:02:15, 11.30s/it]  8%|â–Š         | 81/1048 [15:02<3:01:10, 11.24s/it]  8%|â–Š         | 82/1048 [15:13<3:00:30, 11.21s/it]  8%|â–Š         | 83/1048 [15:24<2:59:48, 11.18s/it]  8%|â–Š         | 84/1048 [15:35<2:59:18, 11.16s/it]  8%|â–Š         | 85/1048 [15:46<2:58:50, 11.14s/it]  8%|â–Š         | 86/1048 [15:57<2:58:29, 11.13s/it]  8%|â–Š         | 87/1048 [16:08<2:58:11, 11.13s/it]  8%|â–Š         | 88/1048 [16:20<2:57:58, 11.12s/it]  8%|â–Š         | 89/1048 [16:31<2:57:44, 11.12s/it]  9%|â–Š         | 90/1048 [16:42<2:57:31, 11.12s/it]                                                     9%|â–Š         | 90/1048 [16:42<2:57:31, 11.12s/it]  9%|â–Š         | 91/1048 [16:53<2:57:20, 11.12s/it]  9%|â–‰         | 92/1048 [17:04<2:57:08, 11.12s/it]  9%|â–‰         | 93/1048 [17:15<2:56:56, 11.12s/it]  9%|â–‰         | 94/1048 [17:26<2:56:46, 11.12s/it]  9%|â–‰         | 95/1048 [17:37<2:56:34, 11.12s/it]  9%|â–‰         | 96/1048 [17:49<2:56:20, 11.11s/it]  9%|â–‰         | 97/1048 [18:00<2:56:08, 11.11s/it]  9%|â–‰         | 98/1048 [18:11<2:55:55, 11.11s/it]  9%|â–‰         | 99/1048 [18:22<2:55:45, 11.11s/it] 10%|â–‰         | 100/1048 [18:33<2:55:34, 11.11s/it]                                                     10%|â–‰         | 100/1048 [18:33<2:55:34, 11.11s/it] 10%|â–‰         | 101/1048 [18:44<2:55:23, 11.11s/it] 10%|â–‰         | 102/1048 [18:55<2:55:11, 11.11s/it] 10%|â–‰         | 103/1048 [19:06<2:54:59, 11.11s/it] 10%|â–‰         | 104/1048 [19:17<2:54:49, 11.11s/it] 10%|â–ˆ         | 105/1048 [19:29<2:54:38, 11.11s/it] 10%|â–ˆ         | 106/1048 [19:40<2:54:25, 11.11s/it] 10%|â–ˆ         | 107/1048 [19:51<2:54:13, 11.11s/it] 10%|â–ˆ         | 108/1048 [20:02<2:54:04, 11.11s/it] 10%|â–ˆ         | 109/1048 [20:13<2:53:52, 11.11s/it] 10%|â–ˆ         | 110/1048 [20:24<2:53:40, 11.11s/it]                                                     10%|â–ˆ         | 110/1048 [20:24<2:53:40, 11.11s/it] 11%|â–ˆ         | 111/1048 [20:35<2:53:29, 11.11s/it] 11%|â–ˆ         | 112/1048 [20:46<2:53:20, 11.11s/it] 11%|â–ˆ         | 113/1048 [20:57<2:53:09, 11.11s/it] 11%|â–ˆ         | 114/1048 [21:09<2:52:55, 11.11s/it] 11%|â–ˆ         | 115/1048 [21:20<2:52:43, 11.11s/it] 11%|â–ˆ         | 116/1048 [21:31<2:52:32, 11.11s/it] 11%|â–ˆ         | 117/1048 [21:42<2:52:22, 11.11s/it] 11%|â–ˆâ–        | 118/1048 [21:53<2:52:12, 11.11s/it] 11%|â–ˆâ–        | 119/1048 [22:04<2:52:03, 11.11s/it] 11%|â–ˆâ–        | 120/1048 [22:15<2:51:51, 11.11s/it]                                                     11%|â–ˆâ–        | 120/1048 [22:15<2:51:51, 11.11s/it] 12%|â–ˆâ–        | 121/1048 [22:26<2:51:42, 11.11s/it] 12%|â–ˆâ–        | 122/1048 [22:37<2:51:30, 11.11s/it] 12%|â–ˆâ–        | 123/1048 [22:49<2:51:20, 11.11s/it] 12%|â–ˆâ–        | 124/1048 [23:00<2:51:12, 11.12s/it] 12%|â–ˆâ–        | 125/1048 [23:11<2:51:00, 11.12s/it] 12%|â–ˆâ–        | 126/1048 [23:22<2:50:49, 11.12s/it] 12%|â–ˆâ–        | 127/1048 [23:33<2:50:39, 11.12s/it] 12%|â–ˆâ–        | 128/1048 [23:44<2:50:27, 11.12s/it] 12%|â–ˆâ–        | 129/1048 [23:55<2:50:16, 11.12s/it] 12%|â–ˆâ–        | 130/1048 [24:06<2:50:04, 11.12s/it]                                                     12%|â–ˆâ–        | 130/1048 [24:06<2:50:04, 11.12s/it] 12%|â–ˆâ–Ž        | 131/1048 [24:17<2:49:55, 11.12s/it] 13%|â–ˆâ–Ž        | 132/1048 [24:29<2:49:42, 11.12s/it] 13%|â–ˆâ–Ž        | 133/1048 [24:40<2:49:31, 11.12s/it] 13%|â–ˆâ–Ž        | 134/1048 [24:51<2:49:17, 11.11s/it] 13%|â–ˆâ–Ž        | 135/1048 [25:02<2:49:05, 11.11s/it] 13%|â–ˆâ–Ž        | 136/1048 [25:13<2:48:55, 11.11s/it] 13%|â–ˆâ–Ž        | 137/1048 [25:24<2:48:42, 11.11s/it] 13%|â–ˆâ–Ž        | 138/1048 [25:35<2:48:31, 11.11s/it] 13%|â–ˆâ–Ž        | 139/1048 [25:46<2:48:19, 11.11s/it] 13%|â–ˆâ–Ž        | 140/1048 [25:57<2:48:09, 11.11s/it]                                                     13%|â–ˆâ–Ž        | 140/1048 [25:57<2:48:09, 11.11s/it] 13%|â–ˆâ–Ž        | 141/1048 [26:09<2:47:59, 11.11s/it] 14%|â–ˆâ–Ž        | 142/1048 [26:20<2:47:49, 11.11s/it] 14%|â–ˆâ–Ž        | 143/1048 [26:31<2:47:38, 11.11s/it] 14%|â–ˆâ–Ž        | 144/1048 [26:42<2:47:25, 11.11s/it] 14%|â–ˆâ–        | 145/1048 [26:53<2:47:13, 11.11s/it] 14%|â–ˆâ–        | 146/1048 [27:04<2:47:02, 11.11s/it] 14%|â–ˆâ–        | 147/1048 [27:15<2:46:53, 11.11s/it] 14%|â–ˆâ–        | 148/1048 [27:26<2:46:42, 11.11s/it] 14%|â–ˆâ–        | 149/1048 [27:37<2:46:31, 11.11s/it] 14%|â–ˆâ–        | 150/1048 [27:49<2:46:20, 11.11s/it]                                                     14%|â–ˆâ–        | 150/1048 [27:49<2:46:20, 11.11s/it] 14%|â–ˆâ–        | 151/1048 [28:00<2:46:09, 11.11s/it] 15%|â–ˆâ–        | 152/1048 [28:11<2:45:59, 11.12s/it] 15%|â–ˆâ–        | 153/1048 [28:22<2:45:49, 11.12s/it]
Aborted!
Traceback (most recent call last):
  File "/zhome/03/c/164482/code/herd/main.py", line 68, in <module>
    main()
  File "/zhome/03/c/164482/code/herd/main.py", line 54, in main
    finetune(model_values, path_values, config, experts, args.peft_strategy, args.is_base, args.use_base, args.only_router, args.all, args.top_k)
  File "/zhome/03/c/164482/code/herd/herd/finetune.py", line 222, in finetune
    _finetune_experts(dataset, tokenizer, config, experts, peft_strategy, use_base, model_values, path_values)
  File "/zhome/03/c/164482/code/herd/herd/finetune.py", line 207, in _finetune_experts
    trainer.train()
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/transformers/trainer.py", line 1591, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/transformers/trainer.py", line 1892, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/transformers/trainer.py", line 2787, in training_step
    self.accelerator.backward(loss)
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/accelerate/accelerator.py", line 1853, in backward
    loss.backward(**kwargs)
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Terminated
