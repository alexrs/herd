Loaded module: cuda/11.6
Loaded dependency [python3/3.11.4]: gcc/12.3.0-binutils-2.40
Loaded dependency [python3/3.11.4]: sqlite3/3.42.0
Loaded module: python3/3.11.4

Loading python3/3.11.4
  Loading requirement: gcc/12.3.0-binutils-2.40 sqlite3/3.42.0
Found cached dataset parquet (/work3/s212722/herd/datasets/cache/alexrs___parquet/alexrs--alpaca-cleaned-30-clusters-a73d614e768ecc90/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
2023-11-01 19:10:28.570 | INFO     | herd.finetune_utils:get_peft_config:30 - Fine-tuning with MoLoRA using 30 experts
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [10:14<10:14, 614.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [12:19<00:00, 326.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [12:19<00:00, 369.57s/it]
2023-11-01 19:23:21.598 | WARNING  | herd.finetune:_finetune_base:72 - Number of experts is not 1 but we are training the base expert, setting it to 1. num_experts: 1
/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.
  warnings.warn(
2023-11-01 19:23:53.061 | INFO     | herd.finetune:_finetune_base:79 - Training expert: base, output_dir: /work3/s212722/herd/meta-llama/Llama-2-7b-hf/alpaca-30/all_linear_paged_adam_4bit/molora/base
wandb: Currently logged in as: alexrs95. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /zhome/03/c/164482/code/herd/wandb/run-20231101_192358-pm3d4fxv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run molora/base
wandb: ⭐️ View project at https://wandb.ai/alexrs95/herd-llama
wandb: 🚀 View run at https://wandb.ai/alexrs95/herd-llama/runs/pm3d4fxv
  0%|          | 0/1294 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
  0%|          | 1/1294 [00:40<14:30:18, 40.39s/it]  0%|          | 2/1294 [00:45<7:05:42, 19.77s/it]   0%|          | 3/1294 [00:51<4:43:40, 13.18s/it]  0%|          | 4/1294 [00:56<3:37:01, 10.09s/it]  0%|          | 5/1294 [01:01<3:00:12,  8.39s/it]  0%|          | 6/1294 [01:07<2:38:03,  7.36s/it]  1%|          | 7/1294 [01:12<2:24:03,  6.72s/it]  1%|          | 8/1294 [01:17<2:14:55,  6.30s/it]  1%|          | 9/1294 [01:23<2:08:49,  6.01s/it]  1%|          | 10/1294 [01:28<2:04:40,  5.83s/it]                                                     1%|          | 10/1294 [01:28<2:04:40,  5.83s/it]  1%|          | 11/1294 [01:34<2:01:52,  5.70s/it]  1%|          | 12/1294 [01:39<1:59:54,  5.61s/it]  1%|          | 13/1294 [01:44<1:58:34,  5.55s/it]  1%|          | 14/1294 [01:50<1:57:37,  5.51s/it]  1%|          | 15/1294 [01:55<1:56:56,  5.49s/it]  1%|          | 16/1294 [02:01<1:56:27,  5.47s/it]  1%|▏         | 17/1294 [02:06<1:56:07,  5.46s/it]  1%|▏         | 18/1294 [02:12<1:55:52,  5.45s/it]  1%|▏         | 19/1294 [02:17<1:55:41,  5.44s/it]  2%|▏         | 20/1294 [02:22<1:55:31,  5.44s/it]                                                     2%|▏         | 20/1294 [02:22<1:55:31,  5.44s/it]  2%|▏         | 21/1294 [02:28<1:55:24,  5.44s/it]  2%|▏         | 22/1294 [02:33<1:55:18,  5.44s/it]  2%|▏         | 23/1294 [02:39<1:55:13,  5.44s/it]  2%|▏         | 24/1294 [02:44<1:55:08,  5.44s/it]  2%|▏         | 25/1294 [02:50<1:55:02,  5.44s/it]  2%|▏         | 26/1294 [02:55<1:54:58,  5.44s/it]  2%|▏         | 27/1294 [03:01<1:54:53,  5.44s/it]  2%|▏         | 28/1294 [03:06<1:54:47,  5.44s/it]  2%|▏         | 29/1294 [03:11<1:54:42,  5.44s/it]  2%|▏         | 30/1294 [03:17<1:54:38,  5.44s/it]                                                     2%|▏         | 30/1294 [03:17<1:54:38,  5.44s/it]  2%|▏         | 31/1294 [03:22<1:54:33,  5.44s/it]  2%|▏         | 32/1294 [03:28<1:54:29,  5.44s/it]  3%|▎         | 33/1294 [03:33<1:54:24,  5.44s/it]  3%|▎         | 34/1294 [03:39<1:54:19,  5.44s/it]  3%|▎         | 35/1294 [03:44<1:54:14,  5.44s/it]  3%|▎         | 36/1294 [03:50<1:54:10,  5.45s/it]  3%|▎         | 37/1294 [03:55<1:54:05,  5.45s/it]  3%|▎         | 38/1294 [04:00<1:53:59,  5.45s/it]  3%|▎         | 39/1294 [04:06<1:53:53,  5.44s/it]  3%|▎         | 40/1294 [04:11<1:53:47,  5.44s/it]                                                     3%|▎         | 40/1294 [04:11<1:53:47,  5.44s/it]  3%|▎         | 41/1294 [04:17<1:53:40,  5.44s/it]  3%|▎         | 42/1294 [04:22<1:53:35,  5.44s/it]  3%|▎         | 43/1294 [04:28<1:53:30,  5.44s/it]  3%|▎         | 44/1294 [04:33<1:53:25,  5.44s/it]  3%|▎         | 45/1294 [04:39<1:53:18,  5.44s/it]  4%|▎         | 46/1294 [04:44<1:53:12,  5.44s/it]  4%|▎         | 47/1294 [04:49<1:53:07,  5.44s/it]  4%|▎         | 48/1294 [04:55<1:53:02,  5.44s/it]  4%|▍         | 49/1294 [05:00<1:52:57,  5.44s/it]  4%|▍         | 50/1294 [05:06<1:52:50,  5.44s/it]                                                     4%|▍         | 50/1294 [05:06<1:52:50,  5.44s/it]  4%|▍         | 51/1294 [05:11<1:52:44,  5.44s/it]  4%|▍         | 52/1294 [05:17<1:52:39,  5.44s/it]  4%|▍         | 53/1294 [05:22<1:52:33,  5.44s/it]  4%|▍         | 54/1294 [05:28<1:52:28,  5.44s/it]  4%|▍         | 55/1294 [05:33<1:52:24,  5.44s/it]  4%|▍         | 56/1294 [05:38<1:52:18,  5.44s/it]  4%|▍         | 57/1294 [05:44<1:52:14,  5.44s/it]  4%|▍         | 58/1294 [05:49<1:52:08,  5.44s/it]  5%|▍         | 59/1294 [05:55<1:52:04,  5.45s/it]  5%|▍         | 60/1294 [06:00<1:51:58,  5.44s/it]                                                     5%|▍         | 60/1294 [06:00<1:51:58,  5.44s/it]  5%|▍         | 61/1294 [06:06<1:51:53,  5.44s/it]  5%|▍         | 62/1294 [06:11<1:51:47,  5.44s/it]  5%|▍         | 63/1294 [06:17<1:51:40,  5.44s/it]  5%|▍         | 64/1294 [06:22<1:51:34,  5.44s/it]  5%|▌         | 65/1294 [06:27<1:51:28,  5.44s/it]  5%|▌         | 66/1294 [06:33<1:51:23,  5.44s/it]  5%|▌         | 67/1294 [06:38<1:51:17,  5.44s/it]  5%|▌         | 68/1294 [06:44<1:51:12,  5.44s/it]  5%|▌         | 69/1294 [06:49<1:51:06,  5.44s/it]  5%|▌         | 70/1294 [06:55<1:51:01,  5.44s/it]                                                     5%|▌         | 70/1294 [06:55<1:51:01,  5.44s/it]  5%|▌         | 71/1294 [07:00<1:50:55,  5.44s/it]  6%|▌         | 72/1294 [07:06<1:50:51,  5.44s/it]  6%|▌         | 73/1294 [07:11<1:50:46,  5.44s/it]  6%|▌         | 74/1294 [07:16<1:50:41,  5.44s/it]  6%|▌         | 75/1294 [07:22<1:50:36,  5.44s/it]  6%|▌         | 76/1294 [07:27<1:50:30,  5.44s/it]  6%|▌         | 77/1294 [07:33<1:50:24,  5.44s/it]  6%|▌         | 78/1294 [07:38<1:50:19,  5.44s/it]  6%|▌         | 79/1294 [07:44<1:50:13,  5.44s/it]  6%|▌         | 80/1294 [07:49<1:50:08,  5.44s/it]                                                     6%|▌         | 80/1294 [07:49<1:50:08,  5.44s/it]  6%|▋         | 81/1294 [07:55<1:50:04,  5.44s/it]  6%|▋         | 82/1294 [08:00<1:49:56,  5.44s/it]  6%|▋         | 83/1294 [08:05<1:49:49,  5.44s/it]  6%|▋         | 84/1294 [08:11<1:49:43,  5.44s/it]  7%|▋         | 85/1294 [08:16<1:49:36,  5.44s/it]  7%|▋         | 86/1294 [08:22<1:49:32,  5.44s/it]  7%|▋         | 87/1294 [08:27<1:49:28,  5.44s/it]  7%|▋         | 88/1294 [08:33<1:49:24,  5.44s/it]  7%|▋         | 89/1294 [08:38<1:49:20,  5.44s/it]  7%|▋         | 90/1294 [08:43<1:49:12,  5.44s/it]                                                     7%|▋         | 90/1294 [08:43<1:49:12,  5.44s/it]  7%|▋         | 91/1294 [08:49<1:49:05,  5.44s/it]  7%|▋         | 92/1294 [08:54<1:49:00,  5.44s/it]  7%|▋         | 93/1294 [09:00<1:48:54,  5.44s/it]  7%|▋         | 94/1294 [09:05<1:48:48,  5.44s/it]  7%|▋         | 95/1294 [09:11<1:48:43,  5.44s/it]  7%|▋         | 96/1294 [09:16<1:48:37,  5.44s/it]  7%|▋         | 97/1294 [09:22<1:48:31,  5.44s/it]  8%|▊         | 98/1294 [09:27<1:48:27,  5.44s/it]  8%|▊         | 99/1294 [09:32<1:48:21,  5.44s/it]  8%|▊         | 100/1294 [09:38<1:48:16,  5.44s/it]                                                      8%|▊         | 100/1294 [09:38<1:48:16,  5.44s/it]  8%|▊         | 101/1294 [09:43<1:48:11,  5.44s/it]  8%|▊         | 102/1294 [09:49<1:48:05,  5.44s/it]  8%|▊         | 103/1294 [09:54<1:48:01,  5.44s/it]  8%|▊         | 104/1294 [10:00<1:47:55,  5.44s/it]  8%|▊         | 105/1294 [10:05<1:47:50,  5.44s/it]  8%|▊         | 106/1294 [10:11<1:47:44,  5.44s/it]  8%|▊         | 107/1294 [10:16<1:47:40,  5.44s/it]  8%|▊         | 108/1294 [10:21<1:47:35,  5.44s/it]  8%|▊         | 109/1294 [10:27<1:47:30,  5.44s/it]  9%|▊         | 110/1294 [10:32<1:47:25,  5.44s/it]                                                      9%|▊         | 110/1294 [10:32<1:47:25,  5.44s/it]  9%|▊         | 111/1294 [10:38<1:47:19,  5.44s/it]  9%|▊         | 112/1294 [10:43<1:47:12,  5.44s/it]  9%|▊         | 113/1294 [10:49<1:47:07,  5.44s/it]  9%|▉         | 114/1294 [10:54<1:47:01,  5.44s/it]  9%|▉         | 115/1294 [11:00<1:46:58,  5.44s/it]  9%|▉         | 116/1294 [11:05<1:46:52,  5.44s/it]  9%|▉         | 117/1294 [11:10<1:46:45,  5.44s/it]  9%|▉         | 118/1294 [11:16<1:46:39,  5.44s/it]  9%|▉         | 119/1294 [11:21<1:46:35,  5.44s/it]  9%|▉         | 120/1294 [11:27<1:46:30,  5.44s/it]                                                      9%|▉         | 120/1294 [11:27<1:46:30,  5.44s/it]  9%|▉         | 121/1294 [11:32<1:46:25,  5.44s/it]  9%|▉         | 122/1294 [11:38<1:46:18,  5.44s/it] 10%|▉         | 123/1294 [11:43<1:46:13,  5.44s/it] 10%|▉         | 124/1294 [11:49<1:46:08,  5.44s/it] 10%|▉         | 125/1294 [11:54<1:46:04,  5.44s/it] 10%|▉         | 126/1294 [11:59<1:45:56,  5.44s/it] 10%|▉         | 127/1294 [12:05<1:45:51,  5.44s/it] 10%|▉         | 128/1294 [12:10<1:45:46,  5.44s/it] 10%|▉         | 129/1294 [12:16<1:45:39,  5.44s/it] 10%|█         | 130/1294 [12:21<1:45:33,  5.44s/it]                                                     10%|█         | 130/1294 [12:21<1:45:33,  5.44s/it] 10%|█         | 131/1294 [12:27<1:45:28,  5.44s/it] 10%|█         | 132/1294 [12:32<1:45:23,  5.44s/it] 10%|█         | 133/1294 [12:38<1:45:17,  5.44s/it] 10%|█         | 134/1294 [12:43<1:45:12,  5.44s/it] 10%|█         | 135/1294 [12:48<1:45:05,  5.44s/it] 11%|█         | 136/1294 [12:54<1:45:00,  5.44s/it] 11%|█         | 137/1294 [12:59<1:44:54,  5.44s/it] 11%|█         | 138/1294 [13:05<1:44:50,  5.44s/it] 11%|█         | 139/1294 [13:10<1:44:44,  5.44s/it] 11%|█         | 140/1294 [13:16<1:44:39,  5.44s/it]                                                     11%|█         | 140/1294 [13:16<1:44:39,  5.44s/it] 11%|█         | 141/1294 [13:21<1:44:33,  5.44s/it] 11%|█         | 142/1294 [13:26<1:44:28,  5.44s/it] 11%|█         | 143/1294 [13:32<1:44:22,  5.44s/it] 11%|█         | 144/1294 [13:37<1:44:16,  5.44s/it] 11%|█         | 145/1294 [13:43<1:44:10,  5.44s/it]                                                     11%|█         | 145/1294 [13:47<1:44:10,  5.44s/it] 11%|█         | 145/1294 [13:47<1:49:15,  5.71s/it]
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: | 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: / 0.004 MB of 0.004 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                    train/epoch ▁▂▂▂▃▄▄▄▅▆▇▇▇██
wandb:              train/global_step ▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:            train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                     train/loss █▃▂▂▃▂▁▂▁▂▂▁▁▃
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 0.11
wandb:              train/global_step 145
wandb:            train/learning_rate 0.0003
wandb:                     train/loss 0.9381
wandb:               train/total_flos 4.740427931634893e+16
wandb:               train/train_loss 0.9316
wandb:            train/train_runtime 833.3285
wandb: train/train_samples_per_second 6.211
wandb:   train/train_steps_per_second 1.553
wandb: 
wandb: 🚀 View run molora/base at: https://wandb.ai/alexrs95/herd-llama/runs/pm3d4fxv
wandb: ️⚡ View job at https://wandb.ai/alexrs95/herd-llama/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk5NjEyNzE2/version_details/v15
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231101_192358-pm3d4fxv/logs
