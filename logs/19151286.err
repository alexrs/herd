Loaded module: cuda/11.6
Loaded dependency [python3/3.11.4]: gcc/12.3.0-binutils-2.40
Loaded dependency [python3/3.11.4]: sqlite3/3.42.0
Loaded module: python3/3.11.4

Loading python3/3.11.4
  Loading requirement: gcc/12.3.0-binutils-2.40 sqlite3/3.42.0
Found cached dataset parquet (/work3/s212722/herd/datasets/cache/alexrs___parquet/alexrs--alpaca-cleaned-10-clusters-e0d2361138237477/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
2023-11-05 10:17:05.919 | INFO     | herd.finetune_utils:get_peft_config:30 - Fine-tuning with MoLoRA using 10 experts
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [04:09<04:09, 249.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [05:26<00:00, 148.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [05:26<00:00, 163.33s/it]
Traceback (most recent call last):
  File "/zhome/03/c/164482/code/herd/main.py", line 78, in <module>
    main()
  File "/zhome/03/c/164482/code/herd/main.py", line 55, in main
    finetune(
  File "/zhome/03/c/164482/code/herd/herd/finetune.py", line 254, in finetune
    _finetune_router(
  File "/zhome/03/c/164482/code/herd/herd/finetune.py", line 162, in _finetune_router
    model_peft = get_peft_model(model, peft_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/peft/mapping.py", line 116, in get_peft_model
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name=adapter_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/peft/peft_model.py", line 1011, in __init__
    super().__init__(model, peft_config, adapter_name)
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/peft/peft_model.py", line 119, in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/peft/tuners/molora/model.py", line 113, in __init__
    super().__init__(model, config, adapter_name)
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 93, in __init__
    self.inject_adapter(self.model, adapter_name)
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 231, in inject_adapter
    self._create_and_replace(peft_config, adapter_name, target, target_name, parent, **optional_kwargs)
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/peft/tuners/molora/model.py", line 210, in _create_and_replace
    self._replace_module(parent, target_name, new_module, target)
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/peft/tuners/molora/model.py", line 228, in _replace_module
    module.to(child.weight.device)
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 464.00 MiB (GPU 0; 39.39 GiB total capacity; 38.57 GiB already allocated; 199.94 MiB free; 38.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
