Loaded module: cuda/11.6
Loaded dependency [python3/3.11.4]: gcc/12.3.0-binutils-2.40
Loaded dependency [python3/3.11.4]: sqlite3/3.42.0
Loaded module: python3/3.11.4

Loading python3/3.11.4
  Loading requirement: gcc/12.3.0-binutils-2.40 sqlite3/3.42.0
Found cached dataset parquet (/work3/s212722/herd/datasets/cache/alexrs___parquet/alexrs--alpaca-cleaned-10-clusters-e0d2361138237477/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
Loading cached processed dataset at /work3/s212722/herd/datasets/cache/alexrs___parquet/alexrs--alpaca-cleaned-10-clusters-e0d2361138237477/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-4764adc7d89acda8.arrow
2023-11-05 16:25:09.582 | INFO     | herd.finetune_utils:get_peft_config:30 - Fine-tuning with MoLoRA using 10 experts
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [14:01<14:01, 841.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [18:52<00:00, 517.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [18:52<00:00, 566.50s/it]
2023-11-05 16:44:07.821 | WARNING  | herd.finetune:_finetune_experts:203 - Number of experts is not 1 but we are training an expert, setting it to 1. num_experts: 1
/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.
  warnings.warn(
2023-11-05 16:44:15.366 | INFO     | herd.finetune:_finetune_experts:235 - Training expert: expert_3, output_dir: /work3/s212722/herd/meta-llama/Llama-2-7b-hf/alpaca-10/q_v_r4/molora/expert_3
wandb: Currently logged in as: alexrs95. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /zhome/03/c/164482/code/herd/wandb/run-20231105_164417-xf28dn9a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run molora/expert_3
wandb: ⭐️ View project at https://wandb.ai/alexrs95/herd-llama
wandb: 🚀 View run at https://wandb.ai/alexrs95/herd-llama/runs/xf28dn9a
  0%|          | 0/744 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
  0%|          | 1/744 [00:15<3:10:10, 15.36s/it]  0%|          | 2/744 [00:23<2:20:34, 11.37s/it]  0%|          | 3/744 [00:32<2:04:44, 10.10s/it]  1%|          | 4/744 [00:41<1:57:17,  9.51s/it]  1%|          | 5/744 [00:49<1:53:11,  9.19s/it]  1%|          | 6/744 [00:58<1:50:40,  9.00s/it]  1%|          | 7/744 [01:07<1:49:04,  8.88s/it]  1%|          | 8/744 [01:15<1:48:01,  8.81s/it]  1%|          | 9/744 [01:24<1:47:20,  8.76s/it]  1%|▏         | 10/744 [01:32<1:46:50,  8.73s/it]                                                    1%|▏         | 10/744 [01:33<1:46:50,  8.73s/it]  1%|▏         | 11/744 [01:41<1:46:29,  8.72s/it]  2%|▏         | 12/744 [01:50<1:46:14,  8.71s/it]  2%|▏         | 13/744 [01:59<1:46:04,  8.71s/it]  2%|▏         | 14/744 [02:07<1:45:55,  8.71s/it]  2%|▏         | 15/744 [02:16<1:45:49,  8.71s/it]  2%|▏         | 16/744 [02:25<1:45:41,  8.71s/it]  2%|▏         | 17/744 [02:33<1:45:34,  8.71s/it]  2%|▏         | 18/744 [02:42<1:45:26,  8.71s/it]  3%|▎         | 19/744 [02:51<1:45:19,  8.72s/it]  3%|▎         | 20/744 [03:00<1:45:12,  8.72s/it]                                                    3%|▎         | 20/744 [03:00<1:45:12,  8.72s/it]  3%|▎         | 21/744 [03:08<1:45:04,  8.72s/it]  3%|▎         | 22/744 [03:17<1:44:57,  8.72s/it]  3%|▎         | 23/744 [03:26<1:44:49,  8.72s/it]  3%|▎         | 24/744 [03:34<1:44:41,  8.72s/it]  3%|▎         | 25/744 [03:43<1:44:34,  8.73s/it]  3%|▎         | 26/744 [03:52<1:44:27,  8.73s/it]  4%|▎         | 27/744 [04:01<1:44:20,  8.73s/it]  4%|▍         | 28/744 [04:09<1:44:12,  8.73s/it]  4%|▍         | 29/744 [04:18<1:44:04,  8.73s/it]  4%|▍         | 30/744 [04:27<1:43:56,  8.73s/it]                                                    4%|▍         | 30/744 [04:27<1:43:56,  8.73s/it]  4%|▍         | 31/744 [04:36<1:43:48,  8.74s/it]  4%|▍         | 32/744 [04:44<1:43:40,  8.74s/it]  4%|▍         | 33/744 [04:53<1:43:32,  8.74s/it]  5%|▍         | 34/744 [05:02<1:43:24,  8.74s/it]  5%|▍         | 35/744 [05:11<1:43:17,  8.74s/it]  5%|▍         | 36/744 [05:19<1:43:09,  8.74s/it]  5%|▍         | 37/744 [05:28<1:43:01,  8.74s/it]  5%|▌         | 38/744 [05:37<1:42:54,  8.75s/it]  5%|▌         | 39/744 [05:46<1:42:48,  8.75s/it]  5%|▌         | 40/744 [05:54<1:42:39,  8.75s/it]                                                    5%|▌         | 40/744 [05:54<1:42:39,  8.75s/it]  6%|▌         | 41/744 [06:03<1:42:30,  8.75s/it]  6%|▌         | 42/744 [06:12<1:42:22,  8.75s/it]  6%|▌         | 43/744 [06:21<1:42:12,  8.75s/it]  6%|▌         | 44/744 [06:29<1:42:04,  8.75s/it]  6%|▌         | 45/744 [06:38<1:41:58,  8.75s/it]  6%|▌         | 46/744 [06:47<1:41:49,  8.75s/it]  6%|▋         | 47/744 [06:56<1:41:39,  8.75s/it]  6%|▋         | 48/744 [07:04<1:41:30,  8.75s/it]  7%|▋         | 49/744 [07:13<1:41:21,  8.75s/it]  7%|▋         | 50/744 [07:22<1:41:13,  8.75s/it]                                                    7%|▋         | 50/744 [07:22<1:41:13,  8.75s/it]  7%|▋         | 51/744 [07:31<1:41:05,  8.75s/it]  7%|▋         | 52/744 [07:39<1:40:56,  8.75s/it]  7%|▋         | 53/744 [07:48<1:40:49,  8.76s/it]  7%|▋         | 54/744 [07:57<1:40:39,  8.75s/it]  7%|▋         | 55/744 [08:06<1:40:30,  8.75s/it]  8%|▊         | 56/744 [08:14<1:40:20,  8.75s/it]  8%|▊         | 57/744 [08:23<1:40:14,  8.75s/it]  8%|▊         | 58/744 [08:31<1:35:45,  8.38s/it]  8%|▊         | 59/744 [08:39<1:36:53,  8.49s/it]  8%|▊         | 60/744 [08:48<1:37:39,  8.57s/it]                                                    8%|▊         | 60/744 [08:48<1:37:39,  8.57s/it]  8%|▊         | 61/744 [08:57<1:38:07,  8.62s/it]  8%|▊         | 62/744 [09:06<1:38:26,  8.66s/it]  8%|▊         | 63/744 [09:14<1:38:36,  8.69s/it]  9%|▊         | 64/744 [09:23<1:38:40,  8.71s/it]  9%|▊         | 65/744 [09:32<1:38:40,  8.72s/it]  9%|▉         | 66/744 [09:41<1:38:37,  8.73s/it]  9%|▉         | 67/744 [09:49<1:38:33,  8.73s/it]  9%|▉         | 68/744 [09:58<1:38:29,  8.74s/it]  9%|▉         | 69/744 [10:07<1:38:24,  8.75s/it]  9%|▉         | 70/744 [10:16<1:38:15,  8.75s/it]                                                    9%|▉         | 70/744 [10:16<1:38:15,  8.75s/it] 10%|▉         | 71/744 [10:24<1:38:07,  8.75s/it] 10%|▉         | 72/744 [10:33<1:37:58,  8.75s/it] 10%|▉         | 73/744 [10:42<1:37:50,  8.75s/it] 10%|▉         | 74/744 [10:51<1:37:42,  8.75s/it] 10%|█         | 75/744 [10:59<1:37:34,  8.75s/it] 10%|█         | 76/744 [11:08<1:37:25,  8.75s/it] 10%|█         | 77/744 [11:17<1:37:16,  8.75s/it] 10%|█         | 78/744 [11:26<1:37:10,  8.75s/it] 11%|█         | 79/744 [11:34<1:37:00,  8.75s/it] 11%|█         | 80/744 [11:43<1:36:51,  8.75s/it]                                                   11%|█         | 80/744 [11:43<1:36:51,  8.75s/it] 11%|█         | 81/744 [11:52<1:36:41,  8.75s/it] 11%|█         | 82/744 [12:01<1:36:32,  8.75s/it] 11%|█         | 83/744 [12:09<1:36:24,  8.75s/it] 11%|█▏        | 84/744 [12:18<1:36:16,  8.75s/it] 11%|█▏        | 85/744 [12:27<1:36:07,  8.75s/it] 12%|█▏        | 86/744 [12:36<1:35:58,  8.75s/it] 12%|█▏        | 87/744 [12:44<1:35:49,  8.75s/it] 12%|█▏        | 88/744 [12:53<1:35:41,  8.75s/it] 12%|█▏        | 89/744 [13:02<1:35:32,  8.75s/it] 12%|█▏        | 90/744 [13:11<1:35:24,  8.75s/it]                                                   12%|█▏        | 90/744 [13:11<1:35:24,  8.75s/it] 12%|█▏        | 91/744 [13:19<1:35:16,  8.75s/it] 12%|█▏        | 92/744 [13:28<1:35:07,  8.75s/it] 12%|█▎        | 93/744 [13:37<1:34:58,  8.75s/it] 13%|█▎        | 94/744 [13:46<1:34:49,  8.75s/it] 13%|█▎        | 95/744 [13:54<1:34:40,  8.75s/it] 13%|█▎        | 96/744 [14:03<1:34:33,  8.76s/it] 13%|█▎        | 97/744 [14:12<1:34:24,  8.75s/it] 13%|█▎        | 98/744 [14:21<1:34:14,  8.75s/it] 13%|█▎        | 99/744 [14:29<1:34:04,  8.75s/it] 13%|█▎        | 100/744 [14:38<1:33:57,  8.75s/it]                                                    13%|█▎        | 100/744 [14:38<1:33:57,  8.75s/it] 14%|█▎        | 101/744 [14:47<1:33:48,  8.75s/it] 14%|█▎        | 102/744 [14:56<1:33:39,  8.75s/it] 14%|█▍        | 103/744 [15:04<1:33:33,  8.76s/it] 14%|█▍        | 104/744 [15:13<1:33:24,  8.76s/it] 14%|█▍        | 105/744 [15:22<1:33:14,  8.76s/it] 14%|█▍        | 106/744 [15:31<1:33:05,  8.75s/it] 14%|█▍        | 107/744 [15:40<1:32:55,  8.75s/it] 15%|█▍        | 108/744 [15:48<1:32:46,  8.75s/it] 15%|█▍        | 109/744 [15:57<1:32:40,  8.76s/it] 15%|█▍        | 110/744 [16:06<1:32:30,  8.75s/it]                                                    15%|█▍        | 110/744 [16:06<1:32:30,  8.75s/it] 15%|█▍        | 111/744 [16:15<1:32:21,  8.75s/it] 15%|█▌        | 112/744 [16:23<1:32:11,  8.75s/it] 15%|█▌        | 113/744 [16:32<1:32:03,  8.75s/it] 15%|█▌        | 114/744 [16:41<1:31:54,  8.75s/it] 15%|█▌        | 115/744 [16:48<1:25:26,  8.15s/it]                                                    15%|█▌        | 115/744 [16:48<1:25:26,  8.15s/it] 15%|█▌        | 115/744 [16:48<1:31:54,  8.77s/it]
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁▁▂▂▇▇▇████
wandb:              train/global_step ▁▂▂▃▄▄▅▆▆▇██
wandb:            train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁
wandb:                     train/loss █▄▄▃▃▃▁▂▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 1.15
wandb:              train/global_step 115
wandb:            train/learning_rate 0.0003
wandb:                     train/loss 0.8537
wandb:               train/total_flos 7.43975476348846e+16
wandb:               train/train_loss 0.89317
wandb:            train/train_runtime 1011.2096
wandb: train/train_samples_per_second 5.886
wandb:   train/train_steps_per_second 0.736
wandb: 
wandb: 🚀 View run molora/expert_3 at: https://wandb.ai/alexrs95/herd-llama/runs/xf28dn9a
wandb: ️⚡ View job at https://wandb.ai/alexrs95/herd-llama/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk5NjEyNzE2/version_details/v15
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231105_164417-xf28dn9a/logs
