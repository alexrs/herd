Loaded module: cuda/11.6
Loaded dependency [python3/3.11.4]: gcc/12.3.0-binutils-2.40
Loaded dependency [python3/3.11.4]: sqlite3/3.42.0
Loaded module: python3/3.11.4

Loading python3/3.11.4
  Loading requirement: gcc/12.3.0-binutils-2.40 sqlite3/3.42.0
Found cached dataset parquet (/work3/s212722/herd/datasets/cache/alexrs___parquet/alexrs--alpaca-cleaned-30-clusters-a73d614e768ecc90/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)
Filter:   0%|          | 0/51760 [00:00<?, ? examples/s]Filter:  35%|███▍      | 18000/51760 [00:00<00:00, 149791.52 examples/s]Filter:  64%|██████▍   | 33000/51760 [00:00<00:00, 58300.20 examples/s] Filter:  83%|████████▎ | 43000/51760 [00:00<00:00, 50069.48 examples/s]Filter:  97%|█████████▋| 50000/51760 [00:00<00:00, 46031.18 examples/s]                                                                       2023-10-28 16:29:38.716 | INFO     | herd.finetune_utils:get_peft_config:30 - Fine-tuning with MoLoRA using 30 experts
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.29s/it]
2023-10-28 16:29:45.054 | WARNING  | herd.finetune:_finetune_experts:174 - Number of experts is not 1 but we are training an expert, setting it to 1. num_experts: 1
Traceback (most recent call last):
  File "/zhome/03/c/164482/code/herd/main.py", line 68, in <module>
    main()
  File "/zhome/03/c/164482/code/herd/main.py", line 54, in main
    finetune(model_values, path_values, config, experts, args.peft_strategy, args.is_base, args.use_base, args.only_router, args.all, args.top_k)
  File "/zhome/03/c/164482/code/herd/herd/finetune.py", line 222, in finetune
    _finetune_experts(dataset, tokenizer, config, experts, peft_strategy, use_base, model_values, path_values)
  File "/zhome/03/c/164482/code/herd/herd/finetune.py", line 177, in _finetune_experts
    model.load_adapter(base_path, "default")
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/peft/peft_model.py", line 593, in load_adapter
    adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py", line 194, in load_peft_weights
    has_remote_safetensors_file = hub_file_exists(
                                  ^^^^^^^^^^^^^^^^
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/peft/utils/hub_utils.py", line 24, in hub_file_exists
    url = hf_hub_url(repo_id=repo_id, filename=filename, repo_type=repo_type, revision=revision)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 110, in _inner_fn
    validate_repo_id(arg_value)
  File "/zhome/03/c/164482/code/herd/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 158, in validate_repo_id
    raise HFValidationError(
huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/work3/s212722/herd/meta-llama/Llama-2-7b-hf/alpaca-30/all_linear_paged_adam_4bit/molora/base'. Use `repo_type` argument if needed.
