[LoraConfig]
lora_alpha=128
lora_dropout=0.1
r=64
bias=none
task_type=CAUSAL_LM
target_modules=q_proj,v_proj

[MoLoraConfig]
lora_alpha=8
lora_dropout=0.1
r=4
bias=none
task_type=CAUSAL_LM
num_experts=10
target_modules=q_proj,v_proj
router_dropout=0.5
; self_attn_use_value=False

[QuantizationConfig]
use_quantization=True
load_in_4bit=True
bnb_4bit_use_double_quant=True
bnb_4bit_quant_type=nf4
bnb_4bit_compute_dtype=bfloat16
; load_in_8bit=True

[TrainingArguments]
num_train_epochs=2
per_device_train_batch_size=4
gradient_accumulation_steps=2
gradient_checkpointing=False
optim=paged_adamw_32bit
logging_steps=10
save_strategy=epoch
learning_rate=3e-4
bf16=True
tf32=False
max_grad_norm=0.3
warmup_ratio=0.1
lr_scheduler_type=constant
output_dir=${Paths:output_dir}
report_to=wandb

[Models]
model=meta-llama/Llama-2-7b-hf
dataset=alexrs/alpaca-cleaned-10-clusters
embeddings_model=thenlper/gte-small
embeddings_max_length=512
; # Max tokens for our embedding model.  This code is really designed for the gte-*
; series, e.g.: https://huggingface.co/thenlper/gte-small
; but could in theory be generated to work with other models I suspect.

[Paths]
base_dir=/work3/s212722/herd
dataset_dir=${base_dir}/datasets
cache_dir=${base_dir}/cache
output_dir=${base_dir}/${Models:model}/alpaca-10/q_v_r4
experts_dir=experts/
experts_file=experts_alpaca_10.json
