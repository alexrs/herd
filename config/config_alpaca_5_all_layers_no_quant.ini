[LoraConfig]
lora_alpha=128
lora_dropout=0.1
r=64
bias=none
task_type=CAUSAL_LM
target_modules=q_proj,up_proj,o_proj,k_proj,down_proj,gate_proj,v_proj

[MoLoraConfig]
lora_alpha=32
lora_dropout=0.1
r=16
bias=none
task_type=CAUSAL_LM
num_experts=5
target_modules=q_proj,up_proj,o_proj,k_proj,down_proj,gate_proj,v_proj
self_attn_router=True
self_attn_hidden_dim=4
self_attn_use_value=False

[QuantizationConfig]
use_quantization=False
load_in_4bit=True
bnb_4bit_use_double_quant=True
bnb_4bit_quant_type=nf4
bnb_4bit_compute_dtype=bfloat16
; load_in_8bit=True

[TrainingArguments]
num_train_epochs=2
per_device_train_batch_size=4
gradient_accumulation_steps=2
gradient_checkpointing=True
optim=paged_adamw_32bit
logging_steps=10
save_strategy=epoch
learning_rate=3e-4
bf16=True
tf32=False
max_grad_norm=0.3
warmup_ratio=0.1
lr_scheduler_type=constant
output_dir=${Paths:output_dir}
report_to=wandb

[Models]
model=meta-llama/Llama-2-7b-hf
dataset=alexrs/alpaca-cleaned-5-clusters
embeddings_model=thenlper/gte-small
embeddings_max_length=512
; # Max tokens for our embedding model.  This code is really designed for the gte-*
; series, e.g.: https://huggingface.co/thenlper/gte-small
; but could in theory be generated to work with other models I suspect.

[Paths]
base_dir=/work3/s212722/herd
dataset_dir=${base_dir}/datasets
cache_dir=${base_dir}/cache
output_dir=${base_dir}/${Models:model}/alpaca-5/all_linear_paged_adam_no_quant
experts_dir=experts/
experts_file=experts_alpaca_5.json
