[LoraConfig]
lora_alpha=256
lora_dropout=0.1
r=128
bias=none
task_type=CAUSAL_LM
target_modules=q_proj,v_proj

[MoLoraConfig]
lora_alpha=32
lora_dropout=0.1
r=16
bias=none
task_type=CAUSAL_LM
num_experts=9
target_modules=q_proj,v_proj
; self_attn_router=True
; self_attn_hidden_dim=4
; self_attn_use_value=True

[QuantizationConfig]
use_quantization=True
load_in_4bit=True
bnb_4bit_use_double_quant=True
bnb_4bit_quant_type=nf4
bnb_4bit_compute_dtype=bfloat16
; load_in_8bit=True

[TrainingArguments]
num_train_epochs=1
per_device_train_batch_size=6
gradient_accumulation_steps=4
gradient_checkpointing=True
optim=paged_adamw_32bit
logging_steps=10
save_strategy=epoch
learning_rate=2e-4
bf16=False
tf32=True
max_grad_norm=0.3
warmup_ratio=0.1
lr_scheduler_type=constant
output_dir=${Paths:output_dir}
report_to=wandb

[Models]
model=meta-llama/Llama-2-7b-hf
dataset=alexrs/molora-dataset-9-clusters
embeddings_model=thenlper/gte-small
embeddings_max_length=512
; # Max tokens for our embedding model.  This code is really designed for the gte-*
; series, e.g.: https://huggingface.co/thenlper/gte-small
; but could in theory be generated to work with other models I suspect.

[Paths]
base_dir=/work3/s212722/herd
dataset_dir=${base_dir}/datasets
cache_dir=${base_dir}/cache
output_dir=${base_dir}/${Models:model}/alpaca-10/q_v_molora_clusters_16
experts_dir=experts/
experts_file=experts_molora_clusters.json
