[LoraConfig]
lora_alpha=16
lora_dropout=0.1
r=64
bias="none"
task_type="CAUSAL_LM"

[TrainingArguments]
num_train_epochs=3
per_device_train_batch_size=4
gradient_accumulation_steps=2
gradient_checkpointing=True
optim="paged_adamw_32bit"
logging_steps=10
save_strategy="epoch"
learning_rate=2e-4
bf16=True
tf32=True
max_grad_norm=0.3
warmup_ratio=0.03
lr_scheduler_type="constant"